# -*- coding: utf-8 -*-
"""Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1thfX8ZI2NZBo-k83081x_ziPMsXENnaj
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers datasets

from transformers import AutoTokenizer
from transformers import DataCollatorForLanguageModeling
from transformers import AutoModelForMaskedLM, TrainingArguments, Trainer, TrainerCallback
import pandas as pd
from datasets import load_dataset
import torch

def read_dataset(dataset_path, label):
  if '.csv' in dataset_path:
    original_df = pd.read_csv(dataset_path, encoding = 'utf-8')

  original_df = original_df.dropna()
  # original_df = original_df[:30] ######################################

  if label:
    data = original_df[original_df.label == label]
  else:
    data = original_df[original_df.label == label]

  label_length = len(data)
  train = data[:int(80/100*label_length)] # commenttext, label
  test = data[int(80/100*label_length):]
  return {'train': train,  'test': test}

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding='max_length', max_length=512)

def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

def prepare_text_files(label):
  d = read_dataset('cleaned.csv', label)
  with open('train.lm', 'w') as f:
    f.writelines([row['commenttext'] + '\n' for _, row in d['train'].iterrows()])

  with open('test.lm', 'w') as f:
    f.writelines([row['commenttext'] + '\n' for _, row in d['test'].iterrows()])

model_checkpoint = 'distilroberta-base'

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
tokenizer.pad_token = tokenizer.eos_token

for label in [0, 1]:
  prepare_text_files(label)

  dataset = load_dataset('text', data_files={'train': ['train.lm'],
                                          'test' : ['test.lm']})


  tokenized_dataset = dataset.map(
      preprocess_function,
      batched=True,
      num_proc=4,
      remove_columns=dataset["train"].column_names)

  block_size = 128

  lm_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=5)
  data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)


  model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)

  training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_steps=1,
    per_device_train_batch_size=32,
  )

  class SaveCallBack(TrainerCallback):
    "A callback that prints a message at the beginning of training"

    def on_epoch_end(self, args, state, control, **kwargs):
        global model
        torch.save(model, str(label)+'.bert_lm')
        print("saved model")

  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=lm_dataset["train"],
      eval_dataset=lm_dataset["test"],
      data_collator=data_collator,
      callbacks=[SaveCallBack]
  )  

  trainer.train()

prefs = ['i', 'hey', 'what', 'why', 'you', 'my', 'uh', 'bro', 'saba',
         'usa', 'sohrab', 'i like', 'i hate', 'i can', 'can you',
         'i would', 'i love', '']

model.to('cpu')
model.eval()

for pref in prefs:
  inp = tokenizer.encode(pref, return_tensors="pt")
  # inp = inp[0][:-1]
  # inp = inp.unsqueeze(0)
  outputs = model.generate(inp, max_length=10, 
                           top_p=0.95, top_k=50,
                           no_repeat_ngram_size=1,
                           do_sample=True, 
                           )
  
  generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
  print(generated)