# -*- coding: utf-8 -*-
"""Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V7hcqtSo_cg5PUn4zdjuBw3VwH7jkBTY
"""

!pip install transformers

import pandas as pd
from torch.utils.data import DataLoader, Dataset
import transformers
import torch

class CommentDataset(Dataset):
  def __init__(self, dataset_path, dataset_name):
    self.dataset_path = dataset_path
    self.dataset_name = dataset_name
    self.read_dataset()
    self.set_tokenizer()
    self.set_max_length()
    self.tokenize_dataset()

  def set_max_length(self):
    self.max_length = 512

  def set_tokenizer(self):
    self.tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')

  def read_dataset(self):
      if '.csv' in self.dataset_path:
        self.original_df = pd.read_csv(self.dataset_path, encoding = 'utf-8')

      self.original_df = self.original_df.dropna()
      #self.original_df = self.original_df[:10] # ino bardarin

      one_label = self.original_df[self.original_df.label == 1]
      zero_label = self.original_df[self.original_df.label == 0]

      one_label_length = len(one_label)
      zero_label_length = len(zero_label)

      if self.dataset_name == 'train':
        cut_one = one_label[:int(80/100*one_label_length)]
        cut_zero = zero_label[:int(80/100*zero_label_length)]

      else: # val
        cut_one = one_label[int(80/100*one_label_length):]
        cut_zero = zero_label[int(80/100*zero_label_length):]

      data = [cut_one, cut_zero]
      self.original_df = pd.concat(data)

      self.comments = self.original_df['commenttext'].tolist()
      self.labels = self.original_df['label'].tolist()

      assert (len(self.comments) == len(self.labels)), 'dataset rows are not equal.'

      self.dataset_size = len(self.comments)

  def tokenize_dataset(self):
    tokenized = self.tokenizer(
      self.comments, 
      return_tensors = 'pt',
      max_length = self.max_length,
      truncation = True,
      padding = 'max_length')
    
    self.input_ids = tokenized['input_ids']
    self.attention_mask = tokenized['attention_mask']

  def __len__(self):
    return self.dataset_size

  def __getitem__(self, idx):
    inp = {
        'input_ids': self.input_ids[idx],
        'attention_mask': self.input_ids[idx]
    }
    return inp, self.labels[idx]

class CommentModel(torch.nn.Module):

  def __init__(self):
    super().__init__()

    self.bert = transformers.AutoModel.from_pretrained('bert-base-uncased')
    self.linear = torch.nn.Linear(768, 1)
    self.sigmoid = torch.nn.Sigmoid()

  def forward(self, x):
    enc = self.bert(**x)
    logits = self.linear(enc['pooler_output'])
    probs = self.sigmoid(logits)

    return probs

def train():
  model.train()

  for index, batch in enumerate(train_dataloader):
    Xs, y = batch
    probs = model(Xs)
    loss = loss_fn(probs.squeeze().type(torch.FloatTensor), y.type(torch.FloatTensor))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (index + 1) % 1 == 0:
      print(f'done {index + 1} from {len(train_dataloader)}')

def evaluate():
  model.eval()
  all_preds, all_labels = [], []

  for index, batch in enumerate(val_dataloader):
    Xs, y = batch
    probs = model(Xs)

    preds = (probs > 0.5) * 1
    # print(preds.squeeze().tolist(), y.tolist())
    all_preds += preds.squeeze().tolist()
    all_labels += y.tolist()

  acc = len([0 for i in range(len(all_preds)) if all_preds[i] == all_labels[i]]) * 100. / len(all_preds)

  print(acc)

train_dataset = CommentDataset('cleaned.csv', 'train')
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_dataset = CommentDataset('cleaned.csv', 'val')
val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)
model = CommentModel()

epochs = 5
loss_fn = torch.nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

for epoch in range(epochs):
  train()
  evaluate()

len(val_dataset)

d

for i in range(8):
  d = train_dataset[i][0]
  tok = train_dataset.tokenizer
  print(max(d['input_ids']))